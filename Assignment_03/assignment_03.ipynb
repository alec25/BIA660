{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Assignment #3: Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import required modules and load the data from the .json file to a Pandas DataFrame. We inspect the DataFrame object to determine its properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>date</th>\n",
       "      <th>helpful</th>\n",
       "      <th>stars</th>\n",
       "      <th>style</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gene H.</td>\n",
       "      <td>We were looking for a decent flashlight for ou...</td>\n",
       "      <td>2018-04-11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>LED Flashlight 2P</td>\n",
       "      <td>Great range for a small package</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cwazy Wabbit</td>\n",
       "      <td>Very nice, works great</td>\n",
       "      <td>2018-04-11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>LED Flashlight 2P</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>David C. Bradford</td>\n",
       "      <td>Sturdy little lights that work every time as a...</td>\n",
       "      <td>2018-04-10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>LED Flashlight 2P</td>\n",
       "      <td>Sturdy little lights that work every time as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>MDD</td>\n",
       "      <td>I bought 2 and they worked great for 2-3 weeks...</td>\n",
       "      <td>2018-03-09</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>LED Flashlight 2P</td>\n",
       "      <td>I bought 2 and they worked great for 2-3 weeks...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>date</th>\n",
       "      <th>helpful</th>\n",
       "      <th>stars</th>\n",
       "      <th>style</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gene H.</td>\n",
       "      <td>We were looking for a decent flashlight for ou...</td>\n",
       "      <td>2018-04-11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>LED Flashlight 2P</td>\n",
       "      <td>Great range for a small package</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cwazy Wabbit</td>\n",
       "      <td>Very nice, works great</td>\n",
       "      <td>2018-04-11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>LED Flashlight 2P</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>David C. Bradford</td>\n",
       "      <td>Sturdy little lights that work every time as a...</td>\n",
       "      <td>2018-04-10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>LED Flashlight 2P</td>\n",
       "      <td>Sturdy little lights that work every time as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>MDD</td>\n",
       "      <td>I bought 2 and they worked great for 2-3 weeks...</td>\n",
       "      <td>2018-03-09</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>LED Flashlight 2P</td>\n",
       "      <td>I bought 2 and they worked great for 2-3 weeks...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "data = pd.read_json(\"Assignment_03/reviews.json\") # Data import \n",
    "data['date'] = data['date'].apply(lambda x: datetime.fromordinal(x)) # Date conversion\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is of the shape `(833, 7)` and its axes are: `['author', 'body', 'date', 'helpful', 'stars', 'style', 'title']`.\n",
    "Lets now try to create a simple bag-of-words binary-classification model for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent term is 'bright' and it occurs 218 times in the training corpus.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "body_list = data['body'].tolist()\n",
    "rating_list = data['stars'].tolist()\n",
    "x_train, x_test, y_train, y_test = train_test_split(body_list, rating_list, test_size=0.2, random_state=1)\n",
    "vectorizer = CountVectorizer(lowercase=True, strip_accents='ascii', stop_words='english', min_df=0.01, max_df=0.9, binary=True)\n",
    "vectorizer = vectorizer.fit(x_train)\n",
    "x_train = vectorizer.transform(x_train) #just do .toarray() here?\n",
    "x_test = vectorizer.transform(x_test)\n",
    "total_word_counts = [sum(word_count) for word_count in zip(*x_train.toarray())]\n",
    "word_frequencies = dict(zip(vectorizer.get_feature_names(), total_word_counts))\n",
    "most_frequent_term = max(word_frequencies, key = word_frequencies.get)\n",
    "print(\"The most frequent term is '\" + most_frequent_term + \"' and it occurs \" + str(word_frequencies[most_frequent_term]) + \" times in the training corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we can say that the L1 regression was superior to the L2 regression in terms of accuracy and number of variables used. Confusion matrixes for these were very similar and uninteresting. I did not include them for aesthetic reasons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall attempt to plot some data points, despite rampant issues with python plotting in Pycharm and within iPython Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17205f4a5c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import plotly.plotly as py\n",
    "# import cufflinks as cf\n",
    "# from plotly.graph_objs import *\n",
    "top_indexes = [i for i, x in enumerate(y_train) if x>=4]\n",
    "bot_indexes = [i for i, x in enumerate(y_train) if x<4]\n",
    "top_reviews = x_train.toarray()[top_indexes]\n",
    "bot_reviews = x_train.toarray()[bot_indexes]\n",
    "top_word_counts = [sum(word_count) for word_count in zip(*top_reviews)]\n",
    "bot_word_counts = [sum(word_count) for word_count in zip(*bot_reviews)]\n",
    "top_bot_word_counts = pd.DataFrame({\"word\": vectorizer.get_feature_names(), \"positive_review\": top_word_counts, \"negative_review\": bot_word_counts})\n",
    "import matplotlib.pyplot as plt\n",
    "top_bot_most = top_bot_word_counts.sort_values('positive_review')[-5:][::-1].append(top_bot_word_counts.sort_values('negative_review')[-5:][::-1]).set_index('word')\n",
    "top_bot_most.drop_duplicates().plot.bar()\n",
    "# a = top_bot_word_counts.sort_values('top', ascending=False)[:5].set_index('word')\n",
    "# b = top_bot_word_counts.sort_values('bot', ascending=False)[:5].set_index('word')\n",
    "# a.append(b).drop_duplicates().plot.bar()\n",
    "# top_bot_word_counts.sort_values('top', ascending=False)[:5].iplot(kind='bar', filename='cufflinks/grouped-bar-chart') #'jupyter/styled_bar'\n",
    "# top_bot_word_counts.sort_values('top', ascending=False)[:5].iplot(kind='bar', filename='jupyter/styled_bar') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply several different simple regressions and evaluate their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 words used in the L1 regression w/ accuracy: 84.43%, 227 words used in the L2 regression w/ accuracy: 83.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "binary_train = [1 if x >=4 else 0 for x in y_train]\n",
    "binary_test = [1 if x >=4 else 0 for x in y_test]\n",
    "lr1 = LogisticRegression(penalty='l1')\n",
    "lr2 = LogisticRegression(penalty='l2')\n",
    "lr1.fit(x_train, binary_train); lr2.fit(x_train, binary_train)\n",
    "words_used1 = len(lr1.coef_[0]) - lr1.coef_[0].tolist().count(0)\n",
    "words_used2 = len(lr2.coef_[0]) - lr2.coef_[0].tolist().count(0)\n",
    "print(str(words_used1)+' words used in the L1 regression w/ accuracy: '+\n",
    "      str(round(100 * lr1.score(x_test, binary_test), 2))+'%, '+\n",
    "      str(words_used2)+' words used in the L2 regression w/ accuracy: '+\n",
    "      str(round(100 * lr2.score(x_test, binary_test), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we can say that the L1 regression was superior to the L2 regression in terms of accuracy and number of variables used. (As a note though, training accuracy was higher for both regressions, of course, with the L2 having a slight advantage, but due to the difference in words used, the signifigance wasn't great enough to counteract that IMHO) Confusion matrixes for these were very similar and uninteresting. I did not include them for aesthetic reasons. \n",
    "\n",
    "Now we can try for a more robust estimator. To do this, I wanted to transform more data from the scraping into numeric variables to imput into my new model. For the style of the item reviewed, I encoded the labels as integers. For the author variable, there were a number of things I could have done with this, but I decided to check if the author was logged in or was reviewing as an anonymous 'Amazon Customer', this was stored as a binary variable. I was going to incorporate the title into the model as an input variable, however, I first checked if there were any blantant givaways and as it turns out, nearly 1/3rd of the titles explicitly mentioned the number of stars the review had. I therefore combined the title and body of the review before vectorizing it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.77"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data['style'])\n",
    "data['style'] = le.transform(data['style'])\n",
    "data['author'] = [1 if x=='Amazon Customer' else 0 for x in data['author']]\n",
    "round(100*sum([1 if 'stars' in x.lower() else 0 for x in data['title']])/len(data['title']),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181 typos in 833 reviews.\n"
     ]
    }
   ],
   "source": [
    "data['text'] = pd.Series(data['title'] + \" \" + data['body'])\n",
    "data = data.drop(columns=['title', 'body'])\n",
    "data['date'] = data['date'].apply(datetime.toordinal)\n",
    "data['date'] = data['date'] - min(data['date'])\n",
    "data['date'] = data['date'] / max(data['date'])\n",
    "from autocorrect import spell\n",
    "import re\n",
    "raw_texts = data['text'].tolist()\n",
    "def process_text(review_text):\n",
    "    digits = sum(1 for let in review_text if let.isdigit())\n",
    "    no_contractions = re.sub(\"[']+\", \"\", review_text)\n",
    "    clean_text = re.sub(\"[!\\\"#$%&()*+,\\-./:;<=>?@[\\]^_`\\\\{|}~]|(\\n)|[0-9]+\", ' ', no_contractions)\n",
    "    new_text = list(map(spell, clean_text.split()))\n",
    "    differences = sum([1 for a, b in zip(new_text, clean_text.split()) if a != b])\n",
    "    return ' '.join(new_text), digits, differences\n",
    "texts, numbers, typos = zip(*list(map(process_text, raw_texts)))\n",
    "print(str(sum(typos))+' typos in '+str(len(typos))+' reviews.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suprizingly low number of typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helpful, numbers | bright, great, small, work\n"
     ]
    }
   ],
   "source": [
    "data['numbers'] = pd.Series(numbers)\n",
    "data['typos'] = pd.Series(typos)\n",
    "stars = data['stars']\n",
    "data = data.drop(columns=['text', 'stars'])\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "word_list = list(map(str.split, texts))\n",
    "from nltk.stem.snowball import EnglishStemmer # Also turns the text lowercase\n",
    "stemmer = EnglishStemmer(ignore_stopwords=True) #ignore_stopwords=False\n",
    "stemmed_word_list = list(map(lambda x: [stemmer.stem(w) for w in x], word_list))\n",
    "stemmed_list = list(map(lambda x: ' '.join(x), stemmed_word_list))\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words='english', norm=None, use_idf=False, max_df=0.99, min_df=2)\n",
    "texts1, texts2, data1, data2, stars1, stars2 = train_test_split(stemmed_list, data, stars, test_size=0.15, random_state=1)\n",
    "tfidf.fit(texts1)\n",
    "tf1 = tfidf.transform(texts1)\n",
    "tf2 = tfidf.transform(texts2)\n",
    "tf1 = pd.DataFrame(tf1.toarray(), index=data1.axes[0])\n",
    "tf2 = pd.DataFrame(tf2.toarray(), index=data2.axes[0])\n",
    "data_in1 = pd.concat([data1, tf1], axis=1)\n",
    "data_in2 = pd.concat([data2, tf2], axis=1)\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(data_in1, stars1)\n",
    "signifigant_features = data_in1.dtypes.index[[i for i, j in enumerate(lasso.coef_) if j != 0]]\n",
    "sig_words = [tfidf.get_feature_names()[i] for i in signifigant_features[2:]]\n",
    "print(', '.join(signifigant_features[:2])+' | '+', '.join(sig_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Lasso only gives us 6 indicator variables (with an Lambda or \"alpha\" of 0.1, with the default value of 1 we get 0 indicator variables. We can see that the model selected 4 of the word vectors and also the number of people that found the review helpful, and the number of numbers in the reviews text or title. The accuracy of the Lasso regression was embarrasingly low and so was not included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.95% training accuracy, 66.4% testing accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alec\\Anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(data1, stars1)\n",
    "def per(x): return(str(round(100*x,2))+'%')\n",
    "print(per(mlp.score(data1, stars1))+' training accuracy, '+per(mlp.score(data2, stars2))+' testing accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for using the default classifier. Also, after doing this I realized I did it without incorporating any word vectors, just using the other extracted and developed featues. IMHO this accuracy is therefore even more commendable. Accuracy is expected to go down from the binary classifier because 5 classes are much harder to classify than just two. In fact the problem is exacerbated by the unbalanced nature of the dataset. Before we dive into that, lets attempt to make the model using the word vectors and see how that turns out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.01% training accuracy, 71.2% testing accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alec\\Anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier() #\n",
    "mlp.fit(data_in1, stars1) #data1 whoops was using this\n",
    "print(per(mlp.score(data_in1, stars1))+' training accuracy, '+per(mlp.score(data_in2, stars2))+' testing accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh my goodness, the difference in training accuracy is staggering (but, on the other hand, so is the sheer number of predictive variables we added). Despite the size of the MLP being the same throughout both (aside from the input layer), which in some sense acts as a bottleneck to overfitting, the nature of neural networks means that this is still a possible issue. If I wanted to dive further into the tradoff between input variables and accuracy I could use Cohens Kappa or something but for now, lets examine the unbalanced nature of the dataset some more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1720980a9e8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sum([1 if x==5 else 0 for x in stars])/ len(stars) #64.47% of the reviews are 5 star reviews\n",
    "stars.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok well that is... terrifying. Lets first try some Lets try some oversampling shall we? Since we're using a neural network, a naive oversampling would simply increase the number of batches in the training and the representation of those exact undersampled classes, I don't think a priori that it would be as helpful for this problem as it would in say... a convolutional net. I think due to the fluid nature of the text vector imputs, it would be more helpful to use a oversampling strategy that interpolates the dataset, for this purpose I chose SMOTE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.82% training accuracy, 64.8% testing accuracy\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE()\n",
    "new_data_in1, new_stars1 = sm.fit_sample(data_in1, stars1)\n",
    "# (len(new_data_in1)-len(data_in1))/len(data_in1) #2.199\n",
    "mlp = MLPClassifier() \n",
    "mlp.fit(new_data_in1, new_stars1) #well its taking quite a bit longer now lol\n",
    "print(per(mlp.score(new_data_in1, new_stars1))+' training accuracy, '+per(mlp.score(data_in2, stars2))+' testing accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001720982C2E8>]], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(new_stars1).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh dear. Training accuracy went from 99.0% to 99.8% despite the increase in the size of our training data by 220% (which, on first impression, seems like it might make it harder to get a good training accuracy). Testing accuracy however, fell from 71.2% to 64.8%. Sad day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x00000172098AAFD0>]], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "naive = RandomOverSampler(random_state=1, ratio=\"minority\")\n",
    "new_data_in1, new_stars1 = naive.fit_sample(data_in1, stars1)\n",
    "new_data_in1, new_stars1 = naive.fit_sample(new_data_in1, new_stars1) #second \n",
    "new_data_in1, new_stars1 = naive.fit_sample(new_data_in1, new_stars1) #third \n",
    "new_data_in1, new_stars1 = naive.fit_sample(new_data_in1, new_stars1) #fourth \n",
    "pd.DataFrame(new_stars1).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.82% training accuracy, 67.2% testing accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.1991525423728815"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier() #takes a while, ugh\n",
    "mlp.fit(new_data_in1, new_stars1)\n",
    "print(per(mlp.score(new_data_in1, new_stars1))+' training accuracy, '+per(mlp.score(data_in2, stars2))+' testing accuracy')\n",
    "(len(new_data_in1)-len(data_in1))/len(data_in1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so everything improved, but lets try that same 4x resampling with SMOTE instead of with the naive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.78% training accuracy, 68.0% testing accuracy\n2.1991525423728815\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTE(k_neighbors=10, m_neighbors=20, random_state=42)\n",
    "new_data_in1, new_stars1 = sm.fit_sample(data_in1, stars1)\n",
    "new_data_in1, new_stars1 = sm.fit_sample(new_data_in1, new_stars1) #second\n",
    "new_data_in1, new_stars1 = sm.fit_sample(new_data_in1, new_stars1) #third\n",
    "new_data_in1, new_stars1 = sm.fit_sample(new_data_in1, new_stars1) #fourth\n",
    "mlp = MLPClassifier(random_state=42) #\n",
    "mlp.fit(new_data_in1, new_stars1) #well its taking quite a bit longer now lol\n",
    "print(per(mlp.score(new_data_in1, new_stars1))+' training accuracy, '+per(mlp.score(data_in2, stars2))+' testing accuracy') #99.78%  69.6%\n",
    "print((len(new_data_in1)-len(data_in1))/len(data_in1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, now lets try changing the MLP network. Scikit-learn's function documentation reccomends using the 'lbfgs' solver instead of ADAM for small datasets so lets see how that turns out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.87% training accuracy, 66.4% testing accuracy\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs', random_state=42) #'lbfgs' reccomended by scikit-learn for small datasets\n",
    "mlp.fit(new_data_in1, new_stars1) \n",
    "print(per(mlp.score(new_data_in1, new_stars1))+' training accuracy, '+per(mlp.score(data_in2, stars2))+' testing accuracy')\n",
    "# print((len(new_data_in1)-len(data_in1))/len(data_in1)) #i'll stop with this now "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok that turned out poor. I also tried non-reccomended/non-default stochastic gradient descent as a solver, but that didnt show any improvement. I tried changing the activation function from ReLU (no reason that would help, but why not) and all the options lead to a decrease in performance. Suprizingly, using linear neurons did not decrease training or test accuracy substantially, which means this problem could likely be solved sufficiently by a large generalized linear model. I did try to do multi-class classification using LASSO regressions and that failed miserably and I don't have much hope for other purely linear models as a result though. I am suprized that Scikit-learn doesn't offer more choices for gradient descent, I mean... 3 choices, really? Most people in the field would likely be able to name at least three times as many from memory. Their selection of activation functions is sufficient, though they don't offer leaky-ReLU or softplus which is a shame, and they waste space on both logistic and tanh which doesn't make much sense as they're to some extent affine transforms of each other. Oh sure there's efficiency advantages and gradient biases and all that, but those are luxuries that can be worried about after we've been given access to leaky-ReLU! \n",
    "\n",
    "I then tried increasing and decreasing the default learning rate by various settings, none of them improved upon the default ADAM learning rate. Seing as we were using ADAM, I tried changing the values of Beta 1 and 2 both increasing and decreasing them. As always, the defaults worked best. I didn't touch epsilon because I'm no fool. \n",
    "\n",
    "I am suprized at the lack of options for this solver. Some options for momentum and nesterov in SGD but no batch size, no advanced activation functions, no fancy gradient descent algorithms. For shame Scikit-learn! I knew I should've used Keras for this!\n",
    "\n",
    "But I digress, let's move on and try something else. Based on the fact that the training accuracy improved when we added the term frequency vectors, but the test accuracy fell, I think its clear that this model is overfitting. To counteract this I could change my model or... I could just raise the error tolerance for the solver. Yeah lets try that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.6% training accuracy, 66.4% testing accuracy\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(tol=0.0002, random_state=42)\n",
    "mlp.fit(new_data_in1, new_stars1)\n",
    "print(per(mlp.score(new_data_in1, new_stars1))+' training accuracy, '+per(mlp.score(data_in2, stars2))+' testing accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idk what's happening, the test accuracy went up to 70.4% in my .py workbook and yet in the iPython notebook it fell. Random states are the same, everything is the same. *sigh* this is why I don't use notebooks. Lets just change the neuron layout and be done with it. Some Netherlands research institute's paper suggested a single hidden layer with 25 nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.43% training accuracy, 64.0% testing accuracy\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(25), tol=0.0002, random_state=42)\n",
    "mlp.fit(new_data_in1, new_stars1)\n",
    "print(per(mlp.score(new_data_in1, new_stars1))+' training accuracy, '+per(mlp.score(data_in2, stars2))+' testing accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's a shame. Lemme try something wider but deeper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.78% training accuracy, 71.2% testing accuracy\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), tol=0.0002, random_state=42)\n",
    "mlp.fit(new_data_in1, new_stars1)\n",
    "print(per(mlp.score(new_data_in1, new_stars1))+' training accuracy, '+per(mlp.score(data_in2, stars2))+' testing accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok that's better I guess. Now lets see how far we can push it. (Apparently not very far. I tried a few basic hidden neuron configurations and I couldn't get to a higher accuracy) SO I guess that's the final model. I tried many different configurations for hidden layers as well as for other hyper parameters for the model. I didn't put them all in here because, lets face it, this document is boring enough as it is. Some of them are in the attached assignment_03_workbook.py file but most of the noninteresting/counterproductive tests were deleted completely. \n",
    "\n",
    "If I were to do this again, I'd probably use Keras and Tenserflow and try a few other configurations other then the multilayer perceptron multi-class classifier. Using a nonlinear (or even linear, by the looks of it) SVM. Possibly a decision tree to be naieve or naieve bayes to be really naieve, also maybe a random forest. I'd also very much like to see the outcome just using k-nearest neighbors. Could probably use word2vec and spend some time scaling certain word vectors and aggregating them in novel ways. Oh well. \n",
    "\n",
    "And so there we have it. Sorry for the lack of images but matplotlib doesn't agree with my setup and it's hard for me to code blindly. If you want to know why the random state was 42? Memes. Please ignore any of the ConvergenceWarnings that delightfully added details of the filepath to GitHub. Does make sense as to why it hadn't conveged yet though, I mean, it was trying to fit it with not even the word frequency info. I'm certain that using a regression of just these variables wouldn't get you an R^2 close to what it would have to be to match the error tolerance, so it's a given that it would run out of iterations. In the future I could also look at various other iteration limits for those examples specifically. Perhaps a solver could be used to find the point at which training accuracy and test accuracy starts to diverge, a good oppertunity for a graph showing the divergence of overfitting in a real scenario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.87% training accuracy, 65.6% testing accuracy\n"
     ]
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I pledge my honor that I have abided by the Stevens Honor System - Alec K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so there we have it. Sorry for the lack of images but matplotlib doesn't agree with my setup and it's hard for me to code blindly. If you want to know why the random state was 42? Memes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so there we have it. Sorry for the lack of images but matplotlib doesn't agree with my setup and it's hard for me to code blindly. If you want to know why the random state was 42? Memes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
